import os, json
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from langchain.memory import ConversationBufferMemory
from dotenv import load_dotenv

# --- Funciones de persistencia ---
def guardar_memoria():
    """Guarda el historial en JSON serializando solo texto."""
    data = memory.load_memory_variables({})
    # Convertir los mensajes en texto plano
    history_text = []
    for msg in data.get("history", []):
        if hasattr(msg, "type") and hasattr(msg, "content"):
            history_text.append({"type": msg.type, "content": msg.content})
    with open(MEMORY_FILE, "w", encoding="utf-8") as f:
        json.dump({"history": history_text}, f, ensure_ascii=False, indent=2)

def cargar_memoria():
    """Carga la memoria desde el archivo JSON si existe."""
    if os.path.exists(MEMORY_FILE):
        with open(MEMORY_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            for msg in data.get("history", []):
                if msg["type"] == "human":
                    memory.chat_memory.add_user_message(msg["content"])
                elif msg["type"] == "ai":
                    memory.chat_memory.add_ai_message(msg["content"])



# --- Ejecutar con memoria persistente ---
def ejecutar_con_memoria(texto):
    """Ejecuta el modelo conservando memoria entre sesiones."""
    history = memory.load_memory_variables({}).get("history", [])
    chain = prompt | llm
    response = chain.invoke({"history": history, "input": texto})
    #memory.save_context({"input": texto}, {"output": response.content})
    #guardar_memoria()  #  Guarda despu茅s de cada interacci贸n
    return response.content.strip()

# --- Configuraci贸n ---
load_dotenv()
os.environ["GOOGLE_API_KEY"] = os.getenv("GOOGLE_API_KEY")

# Modelo
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.7)

# Prompt con memoria
prompt = ChatPromptTemplate.from_messages([
    ("system", "Eres un asistente amable que recuerda toda la conversaci贸n anterior."),
    ("placeholder", "{history}"),
    ("human", "{input}")
])

# Archivo donde se guardar谩 la memoria
MEMORY_FILE = "memoria.json"

# Crear memoria
memory = ConversationBufferMemory(return_messages=True)

# Cargar memoria previa
cargar_memoria()

# --- Prueba ---
#print(ejecutar_con_memoria("Hola, soy Alejandro."))
print(ejecutar_con_memoria("驴Recuerdas c贸mo me llamo?"))
print(ejecutar_con_memoria("驴Qu茅 hablamos antes?"))
print(ejecutar_con_memoria("驴puedes resumir nuestra conversaci贸n?"))